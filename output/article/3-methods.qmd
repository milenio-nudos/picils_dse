---
editor: visual
bibliography: references.bib
---

# Data

This study draws on student data from the two International studies previously presented.

ICILS 2023 employed a two-stage cluster sampling approach. In the first stage, schools with students in the target grade were randomly selected using a probability proportional to size (PPS) method. In the second stage, one intact class was randomly selected within each sampled school, and all students in that class were invited to participate. The final ICILS 2023 sample consists of data from 132,889 grade 8 students from 5,299 schools across 34 countries and one benchmarking participant [@julianfraillon2024].

PISA 2022 target population consists of 15-year-old students attending educational establishments, employing a two-stage stratified sampling design. In the first stage, schools were systematically selected with PPS from national sampling frames. In the second stage, a target number of students (typically 42) were randomly selected with equal probability from within each sampled school. The subset of data used in this research includes the 393,607 students clustered in 52 (mainly OECD) countries who completed the ICT questionnaire.

*Comentario Nico: Falta señalar la cantidad de escuelas que hay en PISA.

# Variables

Both studies include a module on digital self-efficacy in their questionnaires. In ICILS the items are introduced with the phrase *"How well can you do:"* and recoded categories are 1 = I do not think I could do this; 2 = I have never done this, but I could work out how to do it; 3 = Moderately well; 4 = Very well. PISA module preface the items with the question *"To what extent are you able to do the following tasks when using \<digital resources\>?"*, where \<digital resources\> is a placeholder for a term adapted to the target culture. PISA recoded categories are 1 = I cannot do this; 2 = I struggle to do this on my own; 3 = I can do this with a bit of effort; 4 = I can easily do this. Responses as "I don't know" was threated as missing values. A complete comparison of the items from both surveys is presented in [Table @tbl-comparison], highlighting the concordances and distinctions between the studies, and the type of DSE we are assuming to match the item.

We also consider the three-digit ISO country code and the student's sex (recoded as 1 = male and 2 = female) from both datasets.

**¿Country level variables explanation? (GII, HDI, IDI, etc.)**

```{r}
#| label: tbl-comparison
#| tbl-cap: "DSE item comparison between PISA and ICILS."

library(kableExtra)
library(openxlsx)

table <- openxlsx::read.xlsx(here::here("input", "documents", "batteries_icils_pisa_selfeff_comparison.xlsx"), fillMergedCells = TRUE)
table[is.na(table)] <- ""

table |>
  knitr::kable() |>
scroll_box(height="800px")

#Hay que añadir la cantidad de NA por item en esta tabla
```

# Methods

The main analyses were performed within a Confirmatory Factor Analysis (CFA) framework to test the hypothesized two-factor structure of DSE [@brown2015]. All data management and analyses were conducted using the R statistical environment. The CFA models were estimated with the lavaan package [@rosseel2012], and the final multilevel models were estimated with the lme4 package [@bates2015]. Given the ordinal nature of the Likert-scale items, they were treated as ordered categorical variables in the CFA models. This specification handles missing data through pairwise deletion by default.

To evaluate the goodness-of-fit of the pooled CFA model, chi-square (χ²) test was used. However, due to the test's sensitivity to large sample sizes, model evaluation primarily relied on a complementary set of fit indices: the Comparative Fit Index (CFI), the Tucker-Lewis Index (TLI), and the Root Mean Square Error of Approximation (RMSEA). Following common guidelines, CFI and TLI values of 0.95 or higher are considered indicative of excellent fit, while values between 0.90 and 0.95 are considered acceptable. For RMSEA, values of 0.06 or less indicate a close fit, while values up to 0.08 are considered adequate [@brown2015; @wang2012].

To test the comparability of this measurement model across countries and genders, a series of Multi-Group Confirmatory Factor Analyses (MGCFA) was conducted to assess Measurement Equivalence or Invariance (ME/I) [e.g., @beaujean2014; @davidov2014]. This procedure involves testing three sequential levels of invariance by imposing progressive restrictions: **1) configural invariance**, which tests if the same factor structure holds for all groups; **2) metric invariance**, which adds the constraint that factor loadings are equal across groups; and **3) scalar invariance**, which further constrains item intercepts (or thresholds for ordinal items) to be equal [@cheung2002; @milfont2010]. The establishment of invariance between these nested models was evaluated by examining the change in CFI (ΔCFI) and RMSEA (ΔRMSEA). Specifically, for **metric (weak) invariance**, a change in CFI of ≥ -0.004 and a change in RMSEA of ≤ 0.050 were considered acceptable. For **scalar (strong) invariance**, a ΔCFI of ≥ -0.004 and a ΔRMSEA of ≤ 0.010 were considered adequate [@rutkowski2017]. Furthermore, a more demanding criterion of ΔCFI ≥ -0.002 was also considered, as suggested for models with three or fewer dimensions [@rutkowski2017]. Achieving scalar invariance is a prerequisite for further analysis.

Upon establishing scalar invariance, latent mean scores of General and Specialized DSE country distribution are estimated. Then the magnitude of the DSE gender gap is visualized for each country. Finally, correlations between country-level gender gap and a set of national indicators is estimated to explore most salients contextual predictors.